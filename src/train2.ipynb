{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape final dos dados: (90838, 8)\n",
      "Valores únicos no target: count    90838.000000\n",
      "mean        -0.162388\n",
      "std          0.417102\n",
      "min         -0.958333\n",
      "25%         -0.392035\n",
      "50%         -0.172221\n",
      "75%          0.000139\n",
      "max          2.000000\n",
      "Name: target, dtype: float64\n",
      "Score R²: 0.1506\n",
      "Score R²: 0.2112\n",
      "Score R²: 0.1979\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def preparar_dados(df):\n",
    "    \"\"\"Prepara os dados iniciais com tratamento robusto do target\"\"\"\n",
    "    # Converter data\n",
    "    df['data_pregao'] = pd.to_datetime(df['data_pregao'])\n",
    "    \n",
    "    # Ordenar por ação e data\n",
    "    df = df.sort_values(['cod_negociacao', 'data_pregao'])\n",
    "    \n",
    "    # Calcular retorno futuro de forma segura\n",
    "    df['retorno_futuro'] = df.groupby('cod_negociacao')['preco_fechamento'].transform(\n",
    "        lambda x: x.pct_change(periods=52)\n",
    "    )\n",
    "    \n",
    "    # Remover retornos extremos (outliers)\n",
    "    df['retorno_futuro'] = df['retorno_futuro'].clip(\n",
    "        lower=df['retorno_futuro'].quantile(0.01),\n",
    "        upper=df['retorno_futuro'].quantile(0.99)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extrair_features(df):\n",
    "    \"\"\"Extrai features com tratamento completo de infinitos e NA\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Garantir ordenação\n",
    "    df = df.sort_values(['cod_negociacao', 'data_pregao'])\n",
    "    \n",
    "    # Grupo para cálculos\n",
    "    gb = df.groupby('cod_negociacao')['preco_fechamento']\n",
    "    \n",
    "    # Função segura para retornos\n",
    "    def safe_return(current, past, min_price=0.01):\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            return np.where(\n",
    "                (past > min_price) & (current > min_price),\n",
    "                (current - past) / past,\n",
    "                np.nan\n",
    "            )\n",
    "    \n",
    "    # Médias móveis\n",
    "    for window in [4, 8, 12, 26]:\n",
    "        df[f'media_{window}w'] = gb.rolling(window=window).mean().reset_index(level=0, drop=True)\n",
    "        df[f'vol_{window}w'] = gb.rolling(window=window).std().reset_index(level=0, drop=True)\n",
    "    \n",
    "    # Retornos protegidos\n",
    "    for weeks, periods in [(4,3), (12,11), (26,25)]:\n",
    "        preco_passado = gb.shift(periods)\n",
    "        df[f'retorno_{weeks}w'] = safe_return(df['preco_fechamento'], preco_passado)\n",
    "    \n",
    "    # Volume médio\n",
    "    vol_gb = df.groupby('cod_negociacao')['volume']\n",
    "    for window in [4, 12]:\n",
    "        df[f'volume_medio_{window}w'] = vol_gb.rolling(window=window).mean().reset_index(level=0, drop=True)\n",
    "    \n",
    "    # Momentum com fillna\n",
    "    df['momento_4_12'] = (df['retorno_4w'].fillna(0) - df['retorno_12w'].fillna(0))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_data(X, y):\n",
    "    \"\"\"Limpeza final dos dados antes do treino\"\"\"\n",
    "    # Juntar X e y para limpeza consistente\n",
    "    data = X.copy()\n",
    "    data['target'] = y\n",
    "    \n",
    "    # Remover infinitos e NA\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Separar novamente\n",
    "    X_clean = data.drop('target', axis=1)\n",
    "    y_clean = data['target']\n",
    "    \n",
    "    return X_clean, y_clean\n",
    "\n",
    "def treinar_modelo(df):\n",
    "    \"\"\"Função final de treino com todas as proteções\"\"\"\n",
    "    # Preparação dos dados\n",
    "    df = preparar_dados(df)\n",
    "    df = extrair_features(df)\n",
    "    \n",
    "    # Features selecionadas\n",
    "    features = ['media_4w', 'media_12w', 'vol_4w', 'vol_12w', \n",
    "                'retorno_4w', 'retorno_12w', 'volume_medio_4w',\n",
    "                'momento_4_12']\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df['retorno_futuro']\n",
    "    \n",
    "    # Limpeza final\n",
    "    X, y = clean_data(X, y)\n",
    "    \n",
    "    # Pipeline robusto\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            max_depth=5  # Limite de profundidade para evitar overfitting\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Validação cruzada temporal\n",
    "    tscv = TimeSeriesSplit(n_splits=3)  # Reduzido para 3 por performance\n",
    "    \n",
    "    print(f\"Shape final dos dados: {X.shape}\")\n",
    "    print(f\"Valores únicos no target: {pd.Series(y).describe()}\")\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        score = pipeline.score(X_test, y_test)\n",
    "        print(f\"Score R²: {score:.4f}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Carregar dados\n",
    "df_semanal = pd.read_csv(\"semanal2021-2022.csv\")\n",
    "\n",
    "# Treinar modelo\n",
    "modelo = treinar_modelo(df_semanal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados...\n",
      "\n",
      "Preparando target...\n",
      "\n",
      "Extraindo features...\n",
      "\n",
      "Limpando dados...\n",
      "\n",
      "Shape final dos dados: (90752, 12)\n",
      "Distribuição do target:\n",
      "count    90752.000000\n",
      "mean        -0.162085\n",
      "std          0.416844\n",
      "min         -0.958333\n",
      "25%         -0.391588\n",
      "50%         -0.172014\n",
      "75%          0.000265\n",
      "max          2.000000\n",
      "Name: retorno_futuro, dtype: float64\n",
      "\n",
      "Avaliando modelo...\n",
      "\n",
      "Resultados da Validação Cruzada:\n",
      "Fold 1: R² = 0.1395\n",
      "Fold 2: R² = 0.2309\n",
      "Fold 3: R² = 0.2150\n",
      "Média R²: 0.1951 (±0.0399)\n",
      "\n",
      "Treinando modelo final...\n",
      "Modelo não possui atributo de importância de features\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Classe para extração avançada de features sem TA-Lib\"\"\"\n",
    "    def __init__(self):\n",
    "        self.features_to_use = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Garantir ordenação correta\n",
    "        df = df.sort_values(['cod_negociacao', 'data_pregao'])\n",
    "        \n",
    "        # Grupo para cálculos\n",
    "        gb = df.groupby('cod_negociacao')['preco_fechamento']\n",
    "        \n",
    "        # 1. Features básicas\n",
    "        for window in [4, 8, 12, 26]:\n",
    "            df[f'media_{window}w'] = gb.rolling(window=window).mean().reset_index(level=0, drop=True)\n",
    "            df[f'vol_{window}w'] = gb.rolling(window=window).std().reset_index(level=0, drop=True)\n",
    "        \n",
    "        # 2. Retornos com tratamento robusto\n",
    "        def safe_return(current, past, min_price=0.01):\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                return np.where(\n",
    "                    (past > min_price) & (current > min_price),\n",
    "                    (current - past) / past,\n",
    "                    np.nan\n",
    "                )\n",
    "        \n",
    "        for weeks, periods in [(1,0), (4,3), (12,11), (26,25)]:\n",
    "            preco_passado = gb.shift(periods)\n",
    "            df[f'retorno_{weeks}w'] = safe_return(df['preco_fechamento'], preco_passado)\n",
    "        \n",
    "        # 3. Implementação simplificada de RSI sem TA-Lib\n",
    "        def calculate_rsi(series, window=14):\n",
    "            delta = series.diff()\n",
    "            gain = delta.where(delta > 0, 0)\n",
    "            loss = -delta.where(delta < 0, 0)\n",
    "            \n",
    "            avg_gain = gain.rolling(window=window).mean()\n",
    "            avg_loss = loss.rolling(window=window).mean()\n",
    "            \n",
    "            rs = avg_gain / avg_loss\n",
    "            return 100 - (100 / (1 + rs))\n",
    "        \n",
    "        df['rsi_14'] = df.groupby('cod_negociacao')['preco_fechamento'].transform(calculate_rsi)\n",
    "        \n",
    "        # 4. Features temporais\n",
    "        for lag in [1, 2, 3, 4]:\n",
    "            df[f'retorno_lag_{lag}w'] = df.groupby('cod_negociacao')['retorno_1w'].shift(lag)\n",
    "        \n",
    "        df['ema_12'] = df.groupby('cod_negociacao')['preco_fechamento'].transform(\n",
    "            lambda x: x.ewm(span=12).mean()\n",
    "        )\n",
    "        \n",
    "        # 5. Features de volume\n",
    "        vol_gb = df.groupby('cod_negociacao')['volume']\n",
    "        for window in [4, 12]:\n",
    "            df[f'volume_medio_{window}w'] = vol_gb.rolling(window=window).mean().reset_index(level=0, drop=True)\n",
    "        \n",
    "        # 6. Features setoriais/relativas (se disponível)\n",
    "        if 'setor' in df.columns:\n",
    "            df['retorno_relativo'] = df.groupby(['data_pregao', 'setor'])['retorno_4w'].transform('mean')\n",
    "            df['vol_relativa'] = df['vol_4w'] / df.groupby('data_pregao')['vol_4w'].transform('mean')\n",
    "        \n",
    "        # Definir features a serem usadas\n",
    "        self.features_to_use = [\n",
    "            'media_4w', 'media_12w', 'vol_4w', 'vol_12w',\n",
    "            'retorno_1w', 'retorno_4w', 'retorno_12w',\n",
    "            'retorno_lag_1w', 'retorno_lag_2w',\n",
    "            'volume_medio_4w', 'ema_12', 'rsi_14'\n",
    "        ]\n",
    "        \n",
    "        if 'retorno_relativo' in df.columns:\n",
    "            self.features_to_use.extend(['retorno_relativo', 'vol_relativa'])\n",
    "        \n",
    "        return df\n",
    "\n",
    "# [O resto do código permanece igual a partir da função preparar_target()...]\n",
    "\n",
    "def preparar_target(df):\n",
    "    \"\"\"Prepara o target com tratamento robusto\"\"\"\n",
    "    df = df.sort_values(['cod_negociacao', 'data_pregao'])\n",
    "    \n",
    "    # Calcular retorno futuro\n",
    "    df['retorno_futuro'] = df.groupby('cod_negociacao')['preco_fechamento'].transform(\n",
    "        lambda x: x.pct_change(periods=52)\n",
    "    )\n",
    "    \n",
    "    # Tratar outliers extremos\n",
    "    lower = df['retorno_futuro'].quantile(0.01)\n",
    "    upper = df['retorno_futuro'].quantile(0.99)\n",
    "    df['retorno_futuro'] = df['retorno_futuro'].clip(lower, upper)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# [Continue com as outras funções: clean_data, criar_pipeline, avaliar_modelo, plot_feature_importance, main]\n",
    "def clean_data(X, y):\n",
    "    \"\"\"Limpeza final dos dados\"\"\"\n",
    "    data = X.join(y)\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    data = data.dropna()\n",
    "    return data.drop('retorno_futuro', axis=1), data['retorno_futuro']\n",
    "\n",
    "def criar_pipeline():\n",
    "    \"\"\"Cria pipeline de modelagem avançada\"\"\"\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', HistGradientBoostingRegressor(\n",
    "            max_iter=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=7,\n",
    "            early_stopping=True,\n",
    "            random_state=42,\n",
    "            scoring='r2'\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def avaliar_modelo(X, y):\n",
    "    \"\"\"Avaliação robusta com validação cruzada temporal\"\"\"\n",
    "    modelo = criar_pipeline()\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    scores = cross_val_score(\n",
    "        modelo, X, y,\n",
    "        cv=tscv,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nResultados da Validação Cruzada:\")\n",
    "    for i, score in enumerate(scores):\n",
    "        print(f\"Fold {i+1}: R² = {score:.4f}\")\n",
    "    print(f\"Média R²: {np.mean(scores):.4f} (±{np.std(scores):.4f})\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def plot_feature_importance(modelo, feature_names):\n",
    "    \"\"\"Visualização correta da importância das features para qualquer modelo\"\"\"\n",
    "    try:\n",
    "        # Verifica se é um pipeline ou modelo direto\n",
    "        if hasattr(modelo, 'named_steps'):\n",
    "            model = modelo.named_steps['model']\n",
    "        else:\n",
    "            model = modelo\n",
    "        \n",
    "        # Obtém as importâncias de forma genérica\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            importances = np.abs(model.coef_)\n",
    "        else:\n",
    "            print(\"Modelo não possui atributo de importância de features\")\n",
    "            return\n",
    "        \n",
    "        # Ordena as features por importância\n",
    "        indices = np.argsort(importances)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title('Importância das Features')\n",
    "        plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "        plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "        plt.xlabel('Importância Relativa')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao plotar importância: {str(e)}\")\n",
    "\n",
    "def main(file_path):\n",
    "    \"\"\"Função principal para execução completa sem TA-Lib\"\"\"\n",
    "    # 1. Carregar dados\n",
    "    print(\"Carregando dados...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 2. Preparar target\n",
    "    print(\"\\nPreparando target...\")\n",
    "    df = preparar_target(df)\n",
    "    \n",
    "    # 3. Extrair features\n",
    "    print(\"\\nExtraindo features...\")\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    df_features = feature_extractor.transform(df)\n",
    "    \n",
    "    # 4. Selecionar dados finais\n",
    "    X = df_features[feature_extractor.features_to_use]\n",
    "    y = df_features['retorno_futuro']\n",
    "    \n",
    "    # 5. Limpeza final\n",
    "    print(\"\\nLimpando dados...\")\n",
    "    X_clean, y_clean = clean_data(X, y)\n",
    "    \n",
    "    print(f\"\\nShape final dos dados: {X_clean.shape}\")\n",
    "    print(f\"Distribuição do target:\\n{y_clean.describe()}\")\n",
    "    \n",
    "    # 6. Avaliar modelo\n",
    "    print(\"\\nAvaliando modelo...\")\n",
    "    scores = avaliar_modelo(X_clean, y_clean)\n",
    "    \n",
    "    # 7. Treinar modelo final\n",
    "    print(\"\\nTreinando modelo final...\")\n",
    "    modelo = criar_pipeline()\n",
    "    modelo.fit(X_clean, y_clean)\n",
    "    \n",
    "    # 8. Visualizar importância das features\n",
    "    plot_feature_importance(modelo, feature_extractor.features_to_use)\n",
    "    \n",
    "    return modelo, X_clean, y_clean\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Executar pipeline completo sem TA-Lib\n",
    "    modelo_final, X_final, y_final = main(\"datasets/semanal2021-2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados...\n",
      "\n",
      "Preparando target...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Classe para extração avançada de features\"\"\"\n",
    "    def __init__(self):\n",
    "        self.features_to_use = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Garantir ordenação correta\n",
    "        if 'data_pregao' in df.columns:\n",
    "            df['data_pregao'] = pd.to_datetime(df['data_pregao'])\n",
    "            df = df.sort_values(['cod_negociacao', 'data_pregao'])\n",
    "        \n",
    "        # Grupo para cálculos\n",
    "        gb = df.groupby('cod_negociacao')['preco_fechamento']\n",
    "        \n",
    "        # 1. Médias móveis e volatilidade\n",
    "        for window in [4, 8, 12, 26]:\n",
    "            df[f'media_{window}w'] = gb.rolling(window=window).mean().reset_index(level=0, drop=True)\n",
    "            df[f'vol_{window}w'] = gb.rolling(window=window).std().reset_index(level=0, drop=True)\n",
    "        \n",
    "        # 2. Retornos protegidos contra divisão por zero\n",
    "        def safe_return(current, past, min_price=0.01):\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                return np.where(\n",
    "                    (past > min_price) & (current > min_price),\n",
    "                    (current - past) / past,\n",
    "                    np.nan\n",
    "                )\n",
    "        \n",
    "        for weeks, periods in [(1,0), (4,3), (12,11), (26,25)]:\n",
    "            preco_passado = gb.shift(periods)\n",
    "            df[f'retorno_{weeks}w'] = safe_return(df['preco_fechamento'], preco_passado)\n",
    "        \n",
    "        # 3. RSI manual (sem TA-Lib)\n",
    "        def calculate_rsi(series, window=14):\n",
    "            delta = series.diff()\n",
    "            gain = delta.where(delta > 0, 0)\n",
    "            loss = -delta.where(delta < 0, 0)\n",
    "            \n",
    "            avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "            avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "            \n",
    "            rs = avg_gain / avg_loss.replace(0, np.nan)\n",
    "            return 100 - (100 / (1 + rs.replace(np.inf, 100)))\n",
    "        \n",
    "        df['rsi_14'] = df.groupby('cod_negociacao')['preco_fechamento'].transform(calculate_rsi)\n",
    "        \n",
    "        # 4. Features temporais\n",
    "        for lag in [1, 2, 3, 4]:\n",
    "            df[f'retorno_lag_{lag}w'] = df.groupby('cod_negociacao')['retorno_1w'].shift(lag)\n",
    "        \n",
    "        df['ema_12'] = df.groupby('cod_negociacao')['preco_fechamento'].transform(\n",
    "            lambda x: x.ewm(span=12).mean()\n",
    "        )\n",
    "        \n",
    "        # 5. Features de volume\n",
    "        if 'volume' in df.columns:\n",
    "            vol_gb = df.groupby('cod_negociacao')['volume']\n",
    "            for window in [4, 12]:\n",
    "                df[f'volume_medio_{window}w'] = vol_gb.rolling(window=window).mean().reset_index(level=0, drop=True)\n",
    "        \n",
    "        # Definir features a serem usadas\n",
    "        self.features_to_use = [\n",
    "            'media_4w', 'media_12w', 'vol_4w', 'vol_12w',\n",
    "            # 'retorno_1w',\n",
    "            'retorno_4w', 'retorno_12w',\n",
    "            # 'retorno_lag_1w', 'retorno_lag_2w',\n",
    "            'volume_medio_4w', 'ema_12', 'rsi_14'\n",
    "        ]\n",
    "        \n",
    "        return df\n",
    "\n",
    "def preparar_target(df):\n",
    "    \"\"\"Prepara o target com tratamento robusto\"\"\"\n",
    "    df = df.sort_values(['cod_negociacao', 'data_pregao'])\n",
    "    \n",
    "    # Calcular retorno futuro de 52 semanas\n",
    "    df['retorno_futuro'] = df.groupby('cod_negociacao')['preco_fechamento'].transform(\n",
    "        lambda x: x.pct_change(periods=52)\n",
    "    )\n",
    "    \n",
    "    # Remover outliers extremos\n",
    "    if 'retorno_futuro' in df.columns:\n",
    "        lower = df['retorno_futuro'].quantile(0.01)\n",
    "        upper = df['retorno_futuro'].quantile(0.99)\n",
    "        df['retorno_futuro'] = df['retorno_futuro'].clip(lower, upper)\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_data(X, y):\n",
    "    \"\"\"Limpeza final dos dados\"\"\"\n",
    "    data = X.join(y)\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    data = data.dropna()\n",
    "    return data.drop('retorno_futuro', axis=1), data['retorno_futuro']\n",
    "\n",
    "def criar_pipeline_xgb():\n",
    "    \"\"\"Pipeline com XGBoost otimizado\"\"\"\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', XGBRegressor(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            eval_metric='rmse'\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def treinar_avaliar_xgb(X, y):\n",
    "    \"\"\"Treinamento e avaliação personalizada com validação temporal\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    scores = []\n",
    "    modelos = []\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Criar e treinar modelo\n",
    "        xgb = XGBRegressor(\n",
    "            n_estimators=500,  # Número maior pois usamos early stopping\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            early_stopping_rounds=20,\n",
    "            eval_metric='rmse'\n",
    "        )\n",
    "        \n",
    "        # Treinar com early stopping\n",
    "        xgb.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Avaliar\n",
    "        y_pred = xgb.predict(X_val)\n",
    "        score = r2_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "        modelos.append(xgb)\n",
    "        \n",
    "        print(f\"Fold {len(scores)}: R² = {score:.4f} | Melhor iteração: {xgb.best_iteration}\")\n",
    "    \n",
    "    print(f\"\\nR² Médio: {np.mean(scores):.4f} (±{np.std(scores):.4f})\")\n",
    "    return modelos, scores\n",
    "\n",
    "def plot_xgb_importance(modelo, feature_names):\n",
    "    \"\"\"Visualização da importância das features no XGBoost\"\"\"\n",
    "    importances = modelo.feature_importances_\n",
    "    \n",
    "    # Cria DataFrame para melhor visualização\n",
    "    feat_imp = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance')\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(feat_imp)), feat_imp['Importance'], align='center')\n",
    "    plt.yticks(range(len(feat_imp)), feat_imp['Feature'])\n",
    "    plt.title('Importância das Features - XGBoost')\n",
    "    plt.xlabel('Importância Relativa')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main(file_path):\n",
    "    \"\"\"Função principal\"\"\"\n",
    "    try:\n",
    "        # 1. Carregar dados\n",
    "        print(\"Carregando dados...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # 2. Preparar target\n",
    "        print(\"\\nPreparando target...\")\n",
    "        df = preparar_target(df)\n",
    "        \n",
    "        # 3. Extrair features\n",
    "        print(\"\\nExtraindo features...\")\n",
    "        feature_extractor = FeatureExtractor()\n",
    "        df_features = feature_extractor.transform(df)\n",
    "        \n",
    "        # 4. Selecionar dados finais\n",
    "        X = df_features[feature_extractor.features_to_use]\n",
    "        y = df_features['retorno_futuro']\n",
    "        \n",
    "        # 5. Limpeza final\n",
    "        print(\"\\nLimpando dados...\")\n",
    "        X_clean, y_clean = clean_data(X, y)\n",
    "        \n",
    "        print(f\"\\nDados finais: {X_clean.shape[0]} observações\")\n",
    "        print(f\"Features: {X_clean.shape[1]} variáveis\")\n",
    "        \n",
    "        # 6. Treinar e avaliar\n",
    "        print(\"\\nTreinando XGBoost com validação temporal...\")\n",
    "        modelos, scores = treinar_avaliar_xgb(X_clean, y_clean)\n",
    "        \n",
    "        # 7. Treinar modelo final com todos os dados\n",
    "        print(\"\\nTreinando modelo final...\")\n",
    "        modelo_final = XGBRegressor(\n",
    "            n_estimators=int(np.mean([m.best_iteration for m in modelos])) + 20,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        modelo_final.fit(X_clean, y_clean)\n",
    "        \n",
    "        # 8. Visualizar importância\n",
    "        plot_xgb_importance(modelo_final, feature_extractor.features_to_use)\n",
    "\n",
    "        # 9. Salvar o modelo treinado\n",
    "        print(\"\\nSalvando modelo treinado...\")\n",
    "        modelo_final.save_model('modelo_xgb_treinado.json')\n",
    "        \n",
    "        # Salvar também o feature extractor\n",
    "        print(feature_extractor)\n",
    "        print(df_features)\n",
    "        with open('feature_extractor.pkl', 'wb') as file:\n",
    "            pickle.dump(feature_extractor, file)\n",
    "        \n",
    "        print(\"Modelo e feature extractor salvos com sucesso!\")\n",
    "        \n",
    "        return modelo_final, X_clean, y_clean\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nErro: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Executar pipeline\n",
    "    modelo_xgb, X_data, y_data = main(\"datasets/semanal2021-2022.csv\")\n",
    "    \n",
    "    if modelo_xgb is not None:\n",
    "        print(\"\\nModelo XGBoost treinado com sucesso!\")\n",
    "        # Exemplo de previsão\n",
    "        sample_pred = modelo_xgb.predict(X_data.head(1))\n",
    "        print(f\"Exemplo de previsão para a primeira amostra: {sample_pred[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    91321.000000\n",
       "mean        -0.166043\n",
       "std          0.419488\n",
       "min         -0.958333\n",
       "25%         -0.396154\n",
       "50%         -0.173913\n",
       "75%          0.000000\n",
       "max          2.000000\n",
       "Name: retorno_futuro, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_xgb.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
