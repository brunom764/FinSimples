{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0e3f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo colunas com muitos NaNs: ['1. open', '2. high', '3. low', '4. close', '5. adjusted close', '6. volume', '7. dividend amount', '8. split coefficient']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Colaborador\\AppData\\Local\\Temp\\ipykernel_2752\\926823840.py:54: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df['retorno_1D'] = df.groupby('ticker')['close'].pct_change()\n",
      "C:\\Users\\Colaborador\\AppData\\Local\\Temp\\ipykernel_2752\\926823840.py:56: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[f'retorno_{window}D'] = df.groupby('ticker')['close'].pct_change(window)\n",
      "C:\\Users\\Colaborador\\AppData\\Local\\Temp\\ipykernel_2752\\926823840.py:56: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[f'retorno_{window}D'] = df.groupby('ticker')['close'].pct_change(window)\n",
      "C:\\Users\\Colaborador\\AppData\\Local\\Temp\\ipykernel_2752\\926823840.py:56: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[f'retorno_{window}D'] = df.groupby('ticker')['close'].pct_change(window)\n",
      "C:\\Users\\Colaborador\\AppData\\Local\\Temp\\ipykernel_2752\\926823840.py:56: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[f'retorno_{window}D'] = df.groupby('ticker')['close'].pct_change(window)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de NaN por feature:\n",
      "open: 5082 NaNs\n",
      "high: 5082 NaNs\n",
      "low: 5082 NaNs\n",
      "close: 5082 NaNs\n",
      "adjusted_close: 5082 NaNs\n",
      "volume: 5082 NaNs\n",
      "dividend_amount: 5082 NaNs\n",
      "split_coefficient: 5082 NaNs\n",
      "retorno_1D: 5247 NaNs\n",
      "retorno_5D: 5907 NaNs\n",
      "retorno_21D: 8547 NaNs\n",
      "retorno_63D: 15466 NaNs\n",
      "retorno_252D: 46462 NaNs\n",
      "volatilidade_21D: 8547 NaNs\n",
      "volume_avg_21D: 8382 NaNs\n",
      "volume_spike: 13375 NaNs\n",
      "SMA_50: 13167 NaNs\n",
      "SMA_200: 37770 NaNs\n",
      "RSI_14: 9010 NaNs\n",
      "BB_upper: 75326 NaNs\n",
      "BB_lower: 75326 NaNs\n",
      "dividend_yield: 5082 NaNs\n",
      "sector_avg_retorno_21D: 3228 NaNs\n",
      "sector_avg_volume: 1364 NaNs\n",
      "target: 46462 NaNs\n",
      "Total de registros após feature engineering: 283528\n",
      "Setores disponíveis: ['Alimentos' 'Saúde' 'Telecomunicações' 'Agropecuária' 'Indústria Química'\n",
      " 'Logística' 'Educação' 'Varejo' 'Industrial' 'Transportes'\n",
      " 'Construção Civil' 'Financeiro' 'Energia' 'Químico' 'Serviços' 'Consumo'\n",
      " 'Siderurgia' 'Imobiliário' 'Saneamento' 'Holdings' 'Tecnologia'\n",
      " 'Petróleo' 'Mineração' 'Papel e Celulose' 'Distribuição' 'Turismo'\n",
      " 'Metalurgia']\n",
      "Total de registros após remoção de outliers: 256499\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's l1: 0.446549\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.435889\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l1: 0.387873\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's l1: 0.386569\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.339409\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l1: 0.328314\n",
      "[200]\tvalid_0's l1: 0.327508\n",
      "Early stopping, best iteration is:\n",
      "[211]\tvalid_0's l1: 0.327427\n",
      "\n",
      "Métricas médias de validação:\n",
      "MAE: 0.3872\n",
      "RMSE: 0.6895\n",
      "R²: -0.0459\n",
      "\n",
      "Importância das features:\n",
      "feature\n",
      "sector_encoded            74416.576244\n",
      "sector_avg_volume         37344.008717\n",
      "sector_avg_retorno_21D    26397.742116\n",
      "retorno_63D               24732.084634\n",
      "BB_lower                  23437.921458\n",
      "BB_upper                  18556.567571\n",
      "volume_avg_21D            13799.954939\n",
      "SMA_200                   13522.149263\n",
      "volume_spike               9823.888479\n",
      "SMA_50                     6589.993278\n",
      "retorno_21D                 791.330724\n",
      "volatilidade_21D            517.496441\n",
      "RSI_14                      315.238280\n",
      "retorno_5D                   19.773700\n",
      "retorno_1D                    0.000000\n",
      "Name: importance, dtype: float64\n",
      "\n",
      "Modelo treinado com sucesso sem TA-Lib!\n",
      "Arquivos gerados:\n",
      "- modelo_lgbm_sem_talib.txt\n",
      "- metricas_validacao.csv\n",
      "- features_utilizadas.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Carregar e preparar os dados\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath, parse_dates=['Unnamed: 0'], index_col='Unnamed: 0')\n",
    "    df.rename(columns={'setor': 'sector'}, inplace=True)\n",
    "    df = df.sort_index()\n",
    "    df = remover_colunas_com_nans_totais(df)\n",
    "    return df\n",
    "\n",
    "def remover_colunas_com_nans_totais(df):\n",
    "    colunas_para_remover = [\n",
    "        \"1. open\", \"2. high\", \"3. low\", \"4. close\",\n",
    "        \"5. adjusted close\", \"6. volume\",\n",
    "        \"7. dividend amount\", \"8. split coefficient\"\n",
    "    ]\n",
    "\n",
    "    colunas_existentes = [col for col in colunas_para_remover if col in df.columns]\n",
    "\n",
    "    if colunas_existentes:\n",
    "        print(f\"Removendo colunas com muitos NaNs: {colunas_existentes}\")\n",
    "        df = df.drop(columns=colunas_existentes)\n",
    "    else:\n",
    "        print(\"Nenhuma das colunas especificadas foi encontrada.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def print_nan_por_feature(df):\n",
    "    nan_por_coluna = df.isna().sum()\n",
    "    nan_por_coluna = nan_por_coluna[nan_por_coluna > 0]  # Mostra apenas colunas com pelo menos um NaN\n",
    "\n",
    "    if nan_por_coluna.empty:\n",
    "        print(\"Nenhuma feature contém valores NaN.\")\n",
    "    else:\n",
    "        print(\"Quantidade de NaN por feature:\")\n",
    "        for coluna, qtd in nan_por_coluna.items():\n",
    "            print(f\"{coluna}: {qtd} NaNs\")\n",
    "\n",
    "\n",
    "# 2. Feature Engineering sem TA-Lib\n",
    "def create_features(df):\n",
    "    # Codificar setor como variável categórica\n",
    "    le = LabelEncoder()\n",
    "    df['sector_encoded'] = le.fit_transform(df['sector'])\n",
    "    \n",
    "    # Retornos\n",
    "    df['retorno_1D'] = df.groupby('ticker')['close'].pct_change()\n",
    "    for window in [5, 21, 63, 252]:  # 1 semana, 1 mês, 3 meses, 1 ano\n",
    "        df[f'retorno_{window}D'] = df.groupby('ticker')['close'].pct_change(window)\n",
    "    \n",
    "    # Volatilidade\n",
    "    df['volatilidade_21D'] = df.groupby('ticker')['retorno_1D'].rolling(21).std().values\n",
    "    \n",
    "    # Volume\n",
    "    df['volume_avg_21D'] = df.groupby('ticker')['volume'].rolling(21).mean().values\n",
    "    df['volume_avg_21D'] = df['volume_avg_21D'].replace(0, 1)\n",
    "    df['volume_spike'] = df['volume'] / df['volume_avg_21D']\n",
    "    \n",
    "    # Médias Móveis (substituindo TA-Lib)\n",
    "    df['SMA_50'] = df.groupby('ticker')['close'].rolling(50).mean().values\n",
    "    df['SMA_200'] = df.groupby('ticker')['close'].rolling(200).mean().values\n",
    "    \n",
    "    # RSI (implementação manual simplificada)\n",
    "    def calculate_rsi(series, window=14):\n",
    "        delta = series.diff()\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = -delta.where(delta < 0, 0)\n",
    "        \n",
    "        avg_gain = gain.rolling(window).mean()\n",
    "        avg_loss = loss.rolling(window).mean()\n",
    "        \n",
    "        rs = avg_gain / avg_loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    df['RSI_14'] = df.groupby('ticker')['close'].transform(calculate_rsi)\n",
    "    \n",
    "    # Bollinger Bands (implementação manual)\n",
    "    def calculate_bollinger_bands(series, window=20, num_std=2):\n",
    "        sma = series.rolling(window).mean()\n",
    "        std = series.rolling(window).std()\n",
    "        upper = sma + (std * num_std)\n",
    "        lower = sma - (std * num_std)\n",
    "        return upper, lower\n",
    "    \n",
    "    df['BB_upper'], df['BB_lower'] = calculate_bollinger_bands(df['close'])\n",
    "    \n",
    "    # Dividendos\n",
    "    df['dividend_yield'] = df['dividend_amount'] / df['close']\n",
    "    df['dividend_payment'] = (df['dividend_amount'] > 0).astype(int)\n",
    "    \n",
    "    # Features agregadas por setor\n",
    "    df['sector_avg_retorno_21D'] = df.groupby(['sector', df.index])['retorno_21D'].transform('mean')\n",
    "    df['sector_avg_volume'] = df.groupby(['sector', df.index])['volume'].transform('mean')\n",
    "    \n",
    "    # Target: Retorno em 1 ano (252 dias úteis)\n",
    "    df['target'] = df.groupby('ticker')['adjusted_close'].transform(\n",
    "        lambda x: x.shift(-252) / x - 1\n",
    "    )\n",
    "    \n",
    "    print_nan_por_feature(df)\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "# 3. Tratamento de Outliers\n",
    "def remove_outliers(df, z_threshold=3):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['target', 'split_coefficient', 'sector_encoded']]\n",
    "\n",
    "    # Calcula z-score e alinha com os índices do DataFrame\n",
    "    z_scores = np.abs(stats.zscore(df[numeric_cols], nan_policy='omit'))\n",
    "    z_scores_df = pd.DataFrame(z_scores, index=df.index, columns=numeric_cols)\n",
    "\n",
    "    # Diagnóstico\n",
    "    for col in numeric_cols:\n",
    "        pct_removidas = (z_scores_df[col] > z_threshold).mean() * 100\n",
    "        # print(f\"{col}: {pct_removidas:.2f}% das linhas acima do z={z_threshold}\")\n",
    "\n",
    "    # Máscara global: apenas linhas com z-score aceitável em todas as colunas\n",
    "    mask = (z_scores_df < z_threshold).all(axis=1)\n",
    "    df = df[mask]\n",
    "\n",
    "    # Volume mínimo e remoção de NaNs\n",
    "    df = df[df['volume'] > 10000]\n",
    "    return df.dropna()\n",
    "\n",
    "def check_nan_inf(df):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])  # apenas colunas numéricas\n",
    "    result = pd.DataFrame(index=df.columns)\n",
    "\n",
    "    result['NaN'] = df.isna().sum()\n",
    "    result['inf'] = numeric_df.applymap(np.isposinf).sum()\n",
    "    result['-inf'] = numeric_df.applymap(np.isneginf).sum()\n",
    "    result['Total (inf ou NaN)'] = result[['NaN', 'inf', '-inf']].sum(axis=1)\n",
    "\n",
    "    return result[result['Total (inf ou NaN)'] > 0].sort_values('Total (inf ou NaN)', ascending=False)\n",
    "\n",
    "# 4. Treinamento do Modelo\n",
    "def train_model(df):\n",
    "    # Features e target\n",
    "    features = ['retorno_1D', 'retorno_5D', 'retorno_21D', 'retorno_63D',\n",
    "                'volatilidade_21D', 'volume_avg_21D', 'volume_spike',\n",
    "                'RSI_14', 'SMA_50', 'SMA_200', 'BB_upper', 'BB_lower',\n",
    "                'dividend_yield', 'dividend_payment',\n",
    "                'sector_encoded', 'sector_avg_retorno_21D', 'sector_avg_volume']\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df['target']\n",
    "    \n",
    "    # Cross-validation temporal\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Configuração do modelo\n",
    "    params = {\n",
    "        'objective': 'mae',\n",
    "        'num_iterations': 1000,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 7,\n",
    "        'num_leaves': 31,\n",
    "        'random_state': 42,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "    \n",
    "    metrics = []\n",
    "    feature_importances = pd.DataFrame()\n",
    "    models = []  # Para armazenar todos os modelos\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[valid_data],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50),\n",
    "                lgb.log_evaluation(period=100)\n",
    "            ]\n",
    "        )\n",
    "        models.append(model)  # Armazena o modelo\n",
    "        \n",
    "        # Avaliação\n",
    "        preds = model.predict(X_test)\n",
    "        metrics.append({\n",
    "            'Fold': fold + 1,\n",
    "            'MAE': mean_absolute_error(y_test, preds),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, preds)),\n",
    "            'R2': r2_score(y_test, preds)\n",
    "        })\n",
    "        \n",
    "        # Importância das features\n",
    "        fold_importance = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': model.feature_importance(importance_type='gain'),\n",
    "            'fold': fold + 1\n",
    "        })\n",
    "        feature_importances = pd.concat([feature_importances, fold_importance])\n",
    "    \n",
    "    # Métricas médias\n",
    "    avg_metrics = pd.DataFrame(metrics).mean()\n",
    "    print(\"\\nMétricas médias de validação:\")\n",
    "    print(f\"MAE: {avg_metrics['MAE']:.4f}\")\n",
    "    print(f\"RMSE: {avg_metrics['RMSE']:.4f}\")\n",
    "    print(f\"R²: {avg_metrics['R2']:.4f}\")\n",
    "    \n",
    "    # Analisar importância das features\n",
    "    mean_importance = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "    print(\"\\nImportância das features:\")\n",
    "    print(mean_importance.head(15))\n",
    "    \n",
    "    return models[-1], features, pd.DataFrame(metrics)  # Retorna o último modelo treinado\n",
    "\n",
    "# Pipeline completo ajustado\n",
    "if __name__ == \"__main__\":\n",
    "    # Carregar dados\n",
    "    df = load_data('datasets/acoes_consolidadas_todas.csv')\n",
    "    \n",
    "    # Criar features\n",
    "    df = create_features(df)\n",
    "    print(f\"Total de registros após feature engineering: {len(df)}\")\n",
    "    print(f\"Setores disponíveis: {df['sector'].unique()}\")\n",
    "\n",
    "    # Remover outliers\n",
    "    df = remove_outliers(df)\n",
    "    print(f\"Total de registros após remoção de outliers: {len(df)}\")\n",
    "    \n",
    "    # Treinar modelo\n",
    "    model, features, metrics_df = train_model(df)\n",
    "    \n",
    "    # Salvar modelo e resultados\n",
    "    model.save_model('modelo_lgbm_sem_talib.txt')  # Método direto do Booster\n",
    "    metrics_df.to_csv('metricas_validacao.csv', index=False)\n",
    "    pd.Series(features).to_csv('features_utilizadas.csv', index=False)\n",
    "    \n",
    "    print(\"\\nModelo treinado com sucesso sem TA-Lib!\")\n",
    "    print(\"Arquivos gerados:\")\n",
    "    print(\"- modelo_lgbm_sem_talib.txt\")\n",
    "    print(\"- metricas_validacao.csv\")\n",
    "    print(\"- features_utilizadas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0830bbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['open: 0.00% das linhas acima do z=3',\n",
       " 'high: 0.00% das linhas acima do z=3',\n",
       " 'low: 0.00% das linhas acima do z=3',\n",
       " 'close: 0.00% das linhas acima do z=3',\n",
       " 'adjusted_close: 0.00% das linhas acima do z=3',\n",
       " 'volume: 1.59% das linhas acima do z=3',\n",
       " 'dividend_amount: 0.48% das linhas acima do z=3',\n",
       " 'retorno_1D: 0.00% das linhas acima do z=3',\n",
       " 'retorno_5D: 0.00% das linhas acima do z=3',\n",
       " 'retorno_21D: 0.00% das linhas acima do z=3',\n",
       " 'retorno_63D: 0.00% das linhas acima do z=3',\n",
       " 'retorno_252D: 0.00% das linhas acima do z=3',\n",
       " 'volatilidade_21D: 0.03% das linhas acima do z=3',\n",
       " 'volume_avg_21D: 1.75% das linhas acima do z=3',\n",
       " 'volume_spike: 0.00% das linhas acima do z=3',\n",
       " 'SMA_50: 0.07% das linhas acima do z=3',\n",
       " 'SMA_200: 0.27% das linhas acima do z=3',\n",
       " 'RSI_14: 0.00% das linhas acima do z=3',\n",
       " 'BB_upper: 0.03% das linhas acima do z=3',\n",
       " 'BB_lower: 0.03% das linhas acima do z=3',\n",
       " 'dividend_yield: 0.16% das linhas acima do z=3',\n",
       " 'dividend_payment: 1.07% das linhas acima do z=3',\n",
       " 'sector_avg_retorno_21D: 0.01% das linhas acima do z=3',\n",
       " 'sector_avg_volume: 1.79% das linhas acima do z=3']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
